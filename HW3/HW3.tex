% Contributions are much appreciated, in order to contribute to this project, head over to this repository:
% https://github.com/bshramin/uofa-eng-assignment

\documentclass[11pt,letterpaper]{article}
\textwidth 6.5in
\textheight 9.in
\oddsidemargin 0in
\headheight 0in
\usepackage{graphicx}
\usepackage{fancybox}
\usepackage[utf8]{inputenc}
\usepackage{epsfig,graphicx}
\usepackage{multicol,pst-plot}
\usepackage{pstricks}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{eucal}
\usepackage{hyperref}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{pdfpages}
\pagestyle{empty}
\DeclareMathOperator{\tr}{Tr}
\newcommand*{\op}[1]{\check{\mathbf#1}}
\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\braket}[2]{\langle #1 | #2 \rangle}
\newcommand{\mean}[1]{\langle #1 \rangle}
\newcommand{\opvec}[1]{\check{\vec #1}}
\renewcommand{\sp}[1]{$${\begin{split}#1\end{split}}$$}

\usepackage{lipsum}

\usepackage{listings}
\usepackage{soul}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\begin{document}
\pagestyle{plain}


% \begin{flushright}\vspace{-5mm}
% \includegraphics[height=2cm]{logo.png}
% \end{flushright}
 
\begin{center}
\textbf{\Large DSC 210 Numerical Linear Algebra, Fall 2025} \\ \bigskip
\large{Homework Problems for Topic 3: \textit{Eigenvalue Problems}} \\  \bigskip
\begin{flushleft}
    \large{Student Name (PID): Kevin Lin (A69043483)}
\end{flushleft}
\end{center}
\vspace{-4mm}
\rule{\linewidth}{0.1mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \bigskip
\bigskip

\begin{enumerate}

\item[] \fbox{%
\begin{minipage}{0.95\textwidth}
Write your solutions to the following problems by typing them in \LaTeX. Unless otherwise noted by the
problem's instructions, show your work and provide justification for
your answer. Homework is due via Gradescope at \textbf{25th November 2025, 11:59 PM}.
\\
\textbf{Late Policy}: If you submit your homework after the deadline we will apply a late penalty of $10\%$ per day.

\item[] \textbf{Guidelines for Homework Related Questions:}
\begin{enumerate}
    \item As a general rule, we can help you understand the homework problems and explain the material from the corresponding lectures, but we cannot give you the entire solution.
    \item Regarding debugging programming questions: We ask you to do some debugging on your own first, including printing out intermediate values in your algorithms, trying a simpler version of the problem, etc.
    \item We will not be pre-grading the homework, i.e. we wonâ€™t confirm if the answer you have is correct.
\end{enumerate}

\item[] \textbf{AI Usage Policy:}
\begin{enumerate}
    \item Code: You may use LLMs to debug your code; however, you may not use LLMs to generate your entire code, and code must be reviewed and tested.
    \item Writing: You may use LLMs to correct grammar, style and latex issues; however, you may not use LLMs to generate entire solutions, sentences or paragraphs. All writing must be in your own voice.
\end{enumerate}

\item [] \textbf{Academic Integrity Policy:}
\begin{enumerate}
    \item [] The UC San Diego Academic Integrity Policy (formerly the Policy on Integrity of Scholarship) is effective as of September 25, 2023 and applies to any cases originating on or after September 25, 2023. The university expects both faculty and students to honor the policy. For students, this means that all academic work will be done by the individual to whom it's assigned, without unauthorized aid of any kind. If violations of academic integrity occur, the same Sanctioning Guidelines apply regardless of which policy was effective for that case.
    
    For more information on how the policy is implemented, refer to the most current procedures. Remember: When in doubt about what constitutes appropriate collaboration or resource use, please ask TAs before proceeding. It's always better to clarify expectations than to risk an academic integrity violation. Academic integrity violations can have serious consequences for your academic record, and you will get zero grades.
\end{enumerate}



You can access the Homework Template using the following link: \url{https://www.overleaf.com/read/vfhcmsppvskp}

\end{minipage}}
\end{enumerate}

%%%%%%%%%%%%%%%
\clearpage
\begin{enumerate}
%%%%%%%%%%%%%%%

\item[] \textbf{Question 1: Singular value decomposition (25 points)}
\\
Determine the SVD of the following matrices by hand calculation. Make sure to explicitly list $\Sigma, \textrm{U}$, and \textrm{V} in your answer.
\begin{enumerate}[label=(\alph*)]
    \item $\begin{bmatrix}
    5 & 0 \\ 0 & 2
    \end{bmatrix}$  (4 points)
    \item $\begin{bmatrix}
    2 & 0 \\ 0 & 5
    \end{bmatrix}$  (4 points)
    \item $\begin{bmatrix}
    0 & 3 \\ 0 & 0 \\ 0 & 0
    \end{bmatrix}$  (4 points)
    \item $\begin{bmatrix}
    1 & 0 \\ 1 & 0
    \end{bmatrix}$  (4 points)
    \item $\begin{bmatrix}
    2 & 2 \\ 3 & 2
    \end{bmatrix}$  (4 points)
    \item Write Python code to compute SVD of the matrices in parts (a) to (e) and verify that the above results match what you got from code.  (5 points)\\
    \textbf{Hint}: You can use the numpy library function.\\
\end{enumerate}

% \vspace{0.3cm}
\textbf{Solution:} \\
\begin{enumerate}[label=(\alph*)]
    \item $\begin{bmatrix}
    5 & 0 \\ 0 & 2
    \end{bmatrix}$: \\
    $\begin{bmatrix} 5 & 0 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 5 & 0 \\ 0 & 2 \end{bmatrix}^T = \begin{bmatrix} 25 & 0 \\ 0 & 4 \end{bmatrix}$ \\
    $\det\left(\begin{bmatrix} 25 - \lambda & 0 \\ 0 & 4 - \lambda \end{bmatrix}\right) = 0$ \\
    $(25 - \lambda)(4 - \lambda) = 0, \therefore \lambda = 25, 4 \therefore \sigma_1 = 5, \sigma_2 = 2$ \\
    For $\lambda = 25$: \\
    $\begin{bmatrix} 0 & 0 \\ 0 & -21 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0 \therefore x_2 = 0, u_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ \\
    For $\lambda = 4$: \\
    $\begin{bmatrix} 21 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0 \therefore x_1 = 0, u_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ \\
    $\therefore U = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \Sigma = \begin{bmatrix} 5 & 0 \\ 0 & 2 \end{bmatrix}, V^T = V = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
    \item $\begin{bmatrix}
    2 & 0 \\ 0 & 5
    \end{bmatrix}$: \\
    $\begin{bmatrix} 2 & 0 \\ 0 & 5 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 5 \end{bmatrix}^T = \begin{bmatrix} 4 & 0 \\ 0 & 25 \end{bmatrix}$ \\
    $\det\left(\begin{bmatrix} 4 - \lambda & 0 \\ 0 & 25 - \lambda \end{bmatrix}\right) = 0$ \\
    $(4 - \lambda)(25 - \lambda) = 0, \therefore \lambda = 4, 25 \therefore \sigma_1 = 2, \sigma_2 = 5$ \\
    For $\lambda = 4$: \\
    $\begin{bmatrix} 0 & 0 \\ 0 & -21 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0 \therefore x_2 = 0, u_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ \\
    For $\lambda = 25$: \\
    $\begin{bmatrix} 21 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0 \therefore x_1 = 0, u_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ \\
    $\therefore U = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \Sigma = \begin{bmatrix} 2 & 0 \\ 0 & 5 \end{bmatrix}, V^T = V = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
    \item $\begin{bmatrix}
    0 & 3 \\ 0 & 0 \\ 0 & 0
    \end{bmatrix}$: \\
    $\begin{bmatrix} 0 & 3 \\ 0 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 3 \\ 0 & 0 \\ 0 & 0 \end{bmatrix}^T = \begin{bmatrix} 9 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ \\
    $\det\left(\begin{bmatrix} 9 - \lambda & 0 & 0 \\ 0 & -\lambda & 0 \\ 0 & 0 & -\lambda \end{bmatrix}\right) = 0$ \\
    $(9 - \lambda)(-\lambda)(-\lambda) = 0, \therefore \lambda = 9, 0, 0 \therefore \sigma_1 = 3, \sigma_2 = \sigma_3 = 0$ \\
    For $\lambda = 9$: \\
    $\begin{bmatrix} 0 & 0 & 0 \\ 0 & -9 & 0 \\ 0 & 0 & -9 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = 0 \therefore x_2 = 0, x_3 = 0, u_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ \\
    For $\lambda = 0$: \\
    $\begin{bmatrix} 9 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = 0 \therefore x_1 = 0, u_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, u_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$ \\
    $\therefore U = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}, \Sigma = \begin{bmatrix} 3 & 0 \\ 0 & 0 \\ 0 & 0 \end{bmatrix}, V^T = V = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$
    \item $\begin{bmatrix}
    1 & 0 \\ 1 & 0
    \end{bmatrix}$: \\
    $\begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix}^T = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$ \\
    $\det\left(\begin{bmatrix} 1 - \lambda & 1 \\ 1 & 1 - \lambda \end{bmatrix}\right) = 0$ \\
    $(1 - \lambda)(1 - \lambda) - 1 = 0 \implies \lambda^2 - 2\lambda = 0, \therefore \lambda = 0, 2 \therefore \sigma_1 = \sqrt{2}, \sigma_2 = 0$ \\
    For $\lambda = 2$: \\
    $\begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0 \therefore x_1 = x_2, u_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ \\
    For $\lambda = 0$: \\
    $\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0 \therefore x_1 = -x_2, u_2 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix}$ \\
    $\therefore U = \begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{bmatrix}, \Sigma = \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix}, V^T = V = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
    \item $\begin{bmatrix}
    2 & 2 \\ 3 & 2
    \end{bmatrix}$: \\
    $\begin{bmatrix} 2 & 2 \\ 3 & 2 \end{bmatrix} \begin{bmatrix} 2 & 2 \\ 3 & 2 \end{bmatrix}^T = \begin{bmatrix} 8 & 10 \\ 10 & 13 \end{bmatrix}$ \\
    $\det\left(\begin{bmatrix} 8 - \lambda & 10 \\ 10 & 13 - \lambda \end{bmatrix}\right) = 0$ \\
    $(8 - \lambda)(13 - \lambda) - 100 = 0 \implies \lambda^2 - 21\lambda + 4 = 0$ \\
    $\lambda = \frac{21 \pm \sqrt{441 - 16}}{2} = \frac{21 \pm \sqrt{425}}{2} \therefore \sigma_1 = \sqrt{\frac{21 + \sqrt{425}}{2}}, \sigma_2 = \sqrt{\frac{21 - \sqrt{425}}{2}}$ \\
    For $\lambda = \frac{21 + \sqrt{425}}{2}$: \\
    $\begin{bmatrix} 8 - \frac{21 + \sqrt{425}}{2} & 10 \\ 10 & 13 - \frac{21 + \sqrt{425}}{2} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0$ \\
    $\begin{bmatrix} \frac{-5 - \sqrt{425}}{2} & 10 \\ 10 & \frac{5 - \sqrt{425}}{2} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0$ \\
    $\begin{bmatrix} 1 & \frac{20}{-5 - \sqrt{425}} \\ 10 & \frac{5 - \sqrt{425}}{2} \end{bmatrix}$ \\
    Note that $\frac{20}{-5 - \sqrt{425}} = \frac{20}{-5 - \sqrt{425}} \cdot \frac{-5 + \sqrt{425}}{-5 + \sqrt{425}} = \frac{20(-5(1 - \sqrt{17}))}{25 - 425} = \frac{-100(1 - \sqrt{17})}{-400} = \frac{1 - \sqrt{17}}{4}$, so we can simplify to: \\
    $\begin{bmatrix} 1 & \frac{1 - \sqrt{17}}{4} \\ 10 & \frac{5(1 - \sqrt{17})}{2} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0, \therefore x_1 = -\frac{1 - \sqrt{17}}{4} x_2, u_1 = \begin{bmatrix} -\frac{1 - \sqrt{17}}{4} \\ 1 \end{bmatrix}$ \\
    We normalize $u_1$ to get: \\
    $u_1 = \frac{1}{\sqrt{\left(-\frac{1 - \sqrt{17}}{4}\right)^2 + 1}} \begin{bmatrix} -\frac{1 - \sqrt{17}}{4} \\ 1 \end{bmatrix} = \frac{1}{\sqrt{\frac{17 - \sqrt{17}}{8}}} \begin{bmatrix} -\frac{1 - \sqrt{17}}{4} \\ 1 \end{bmatrix} = \begin{bmatrix} -\frac{\sqrt{8}(1 - \sqrt{17})}{4\sqrt{17 - \sqrt{17}}} \\ \frac{1}{\sqrt{\frac{17 - \sqrt{17}}{8}}} \end{bmatrix} = \begin{bmatrix} -\frac{\sqrt{2} - \sqrt{34}}{2\sqrt{17 - \sqrt{17}}} \\ \sqrt{\frac{8}{17 - \sqrt{17}}} \end{bmatrix}$ \\
    By same convention for $\lambda = \frac{21 - \sqrt{425}}{2}$, we get $u_2 = \begin{bmatrix} -\frac{\sqrt{2} + \sqrt{34}}{2\sqrt{17 + \sqrt{17}}} \\ \sqrt{\frac{8}{17 + \sqrt{17}}} \end{bmatrix}$ \\
    $\therefore U = \begin{bmatrix} -\frac{\sqrt{2} - \sqrt{34}}{2\sqrt{17 - \sqrt{17}}} & -\frac{\sqrt{2} + \sqrt{34}}{2\sqrt{17 + \sqrt{17}}} \\ \sqrt{\frac{8}{17 - \sqrt{17}}} & \sqrt{\frac{8}{17 + \sqrt{17}}} \end{bmatrix}, \Sigma = \begin{bmatrix} \sqrt{\frac{21 + \sqrt{425}}{2}} & 0 \\ 0 & \sqrt{\frac{21 - \sqrt{425}}{2}} \end{bmatrix}$ \\ 
    We use $v_i = \frac{1}{\sigma_i} A^T u_i$ to compute $V$: \\
    $V = \begin{bmatrix} \frac{\sqrt{17} + 5}{2\sqrt{4\sqrt{17} + 17}} & \frac{-\sqrt{17} + 5}{2\sqrt{-4\sqrt{17} + 17}} \\ \frac{\sqrt{17} + 3}{2\sqrt{4\sqrt{17} + 17}} & \frac{-\sqrt{17} + 3}{2\sqrt{-4\sqrt{17} + 17}} \end{bmatrix}$
    \item Python code:
\begin{lstlisting}[language=python]
import numpy as np

def svd(A):
    u, s, vh = np.linalg.svd(A)
    print("U matrix:\n", u)
    print("Singular values:\n", s)
    print("V^T matrix:\n", vh)

a = np.array([[5, 0], [0, 2]])
b = np.array([[2, 0], [0, 5]])
c = np.array([[0, 3], [0, 0], [0, 0]])
d = np.array([[1, 0], [1, 0]])
e = np.array([[2 ,2], [3, 2]])

print("(a):")
svd(a)
print("\n(b):")
svd(b)
print("\n(c):")
svd(c)
print("\n(d):")
svd(d)
print("\n(e):")
svd(e)
\end{lstlisting}
\begin{verbatim}
(a):
U matrix:
 [[1. 0.]
 [0. 1.]]
Singular values:
 [5. 2.]
V^T matrix:
 [[1. 0.]
 [0. 1.]]

(b):
U matrix:
 [[0. 1.]
 [1. 0.]]
Singular values:
 [5. 2.]
V^T matrix:
 [[0. 1.]
 [1. 0.]]

(c):
U matrix:
 [[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]
Singular values:
 [3. 0.]
V^T matrix:
 [[ 0.  1.]
 [-1.  0.]]

(d):
U matrix:
 [[-0.70710678 -0.70710678]
 [-0.70710678  0.70710678]]
Singular values:
 [1.41421356 0.        ]
V^T matrix:
 [[-1. -0.]
 [ 0.  1.]]

(e):
U matrix:
 [[-0.61541221 -0.78820544]
 [-0.78820544  0.61541221]]
Singular values:
 [4.56155281 0.43844719]
V^T matrix:
 [[-0.78820544 -0.61541221]
 [ 0.61541221 -0.78820544]]
\end{verbatim}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%
\clearpage
\item[] \textbf{Question 2: Eigenvalues and Eigenvectors (25 points)}
%%%%%%%%%%%%%%%%%%%%

\begin{equation*}
    \mathbf{A} 
    = \begin{bmatrix}
        -5   & 3\\
        -6   & 6
    \end{bmatrix}
\end{equation*}

\begin{enumerate}[label=(\alph*)]
    \item Write down the characteristic equation for matrix $\mathbf{A}$. Use the characteristic equation to solve the eigenvalues and normalized eigenvectors of matrix $\mathbf{A}$. (10 points)
    \item Write python code to verify your answers for the eigenvalues and normalized eigenvectors from Part (a). \textbf{Hint}: You can use numpy to solve this problem. (5 points)
    
    \item Prove that if a real matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ has unique eigenvalues, then the eigenvectors $\mathbf{x}_i$ are linearly independent. \textbf{Hint:} Prove by contradiction, start with $n=2$ case. (10 points)\\
\end{enumerate}

% \vspace{0.3cm}
\textbf{Solution:} \\
\begin{enumerate}[label=(\alph*)]
    \item Characteristic equation: \\
    $\det(\mathbf{A} - \lambda \mathbf{I}) = 0$ \\
    $\det\left(\begin{bmatrix} -5 - \lambda & 3 \\ -6 & 6 - \lambda \end{bmatrix}\right) = 0$ \\
    $(-5 - \lambda)(6 - \lambda) + 18 = 0$ \\
    $\lambda^2 - \lambda - 12 = 0$ \\
    $(\lambda - 4)(\lambda + 3) = 0, \therefore \lambda_1 = 4, \lambda_2 = -3$ \\
    For $\lambda_1 = 4$: \\
    $\begin{bmatrix} -9 & 3 \\ -6 & 2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0 \therefore x_2 = 3x_1, u_1 = \frac{1}{\sqrt{10}} \begin{bmatrix} 1 \\ 3 \end{bmatrix}$ \\
    For $\lambda_2 = -3$: \\
    $\begin{bmatrix} -2 & 3 \\ -6 & 9 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0 \therefore x_2 = 3x_1/2, u_2 = \frac{1}{\sqrt{13}} \begin{bmatrix} 2 \\ 3 \end{bmatrix}$s
    \item 
\begin{lstlisting}[language=python]
A = np.array([[-5, 3], [-6, 6]])
eigenvalues, eigenvectors = np.linalg.eig(A)
print("Eigenvalues:\n", eigenvalues)
print("Eigenvectors:\n", eigenvectors)
print(1/np.sqrt(10), 3/np.sqrt(10))
print(2/np.sqrt(13), 3/np.sqrt(13))
\end{lstlisting}
\begin{verbatim}
Eigenvalues:
 [-3.  4.]
Eigenvectors:
 [[-0.83205029 -0.31622777]
 [-0.5547002  -0.9486833 ]]
0.31622776601683794 0.9486832980505138
0.5547001962252291 0.8320502943378437
\end{verbatim}
    \item Start with the $n = 2$ case. Assume that the eigenvectors $\mathbf{x}_1$ and $\mathbf{x}_2$ are linearly dependent. Then, there exists a scalar $c$ such that $\mathbf{x}_2 = c \mathbf{x}_1$. \\
    From the definition of eigenvectors, we have: \\
    $\mathbf{A} \mathbf{x}_1 = \lambda_1 \mathbf{x}_1$ \\
    $\mathbf{A} \mathbf{x}_2 = \lambda_2 \mathbf{x}_2$ \\
    Substituting $\mathbf{x}_2 = c \mathbf{x}_1$ into the second equation: \\
    $\mathbf{A} (c \mathbf{x}_1) = \lambda_2 (c \mathbf{x}_1)$ \\
    $c \mathbf{A} \mathbf{x}_1 = c \lambda_2 \mathbf{x}_1$ \\
    Dividing both sides by $c$ (assuming $c \neq 0$): \\
    $\mathbf{A} \mathbf{x}_1 = \lambda_2 \mathbf{x}_1$ \\
    But from the first equation, we have $\mathbf{A} \mathbf{x}_1 = \lambda_1 \mathbf{x}_1$. \\
    Therefore, $\lambda_1 \mathbf{x}_1 = \lambda_2 \mathbf{x}_1$. \\
    Since $\mathbf{x}_1$ is non-zero, we can divide both sides by $\mathbf{x}_1$: \\
    $\lambda_1 = \lambda_2$ \\
    This contradicts our assumption that $\lambda_1$ and $\lambda_2$ are unique 
    eigenvalues. Therefore, our initial assumption that $\mathbf{x}_1$ and 
    $\mathbf{x}_2$ are linearly dependent must be false. Hence, for $n = 2$, the 
    eigenvectors corresponding to unique eigenvalues are linearly independent. 
    The proof can be extended to $n > 2$ using induction, following a similar 
    argument.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%
\clearpage
\item[] \textbf{Question 3: Power method (20 points)} \\
%%%%%%%%%%%%%%%%%%%%

Solve the problem in the \href{https://drive.google.com/file/d/1cOb-zLjKqcIPwESdG55NyQ26GIms09Z2/view?usp=drive_link}{google colab}. Please make a copy of the notebook and solve it there.

\begin{enumerate}[label=(\alph*)]
\item Power Method:
Write function \texttt{power\_method(A,x)}, which takes as input matrix $A$ and a vector $\mathbf{x}$, and uses the power method to calculate eigenvalues and eigenvectors.

Get the largest \textbf{(in absolute value)} eigenvalue and the corresponding eigenvector for matrix A using the above function.
$$A=\begin{bmatrix}
2 & 2 & 1 \\
1 & 3 & 2 \\
2 & 4 & 1
\end{bmatrix} $$

Start with initial eigenvector guesses: $[-1, 0.5, 3]$ and $[2,-6,0.2]$. For each of the vectors, iterate until convergence.
\begin{enumerate}[label=\roman*.]
    \item Plot how the eigenvalue changes with respect to iterations.
    \item Report the number of steps it took to converge, for both the eigenvalue and eigenvector.
    \item Report the final eigenvalue and eigenvector. Match your output with the results generated by the numpy API: \texttt{numpy.linalg.eig}.\\
\end{enumerate}

\emph{Note}: You only need to look at magnitudes of eigenvalues. Use an absolute tolerance of  $10^{-6}$  between eigenvalue output of previous and current iteration as stopping criteria. You may also need to normalize the final eigenvector to match with output of numpy API \texttt{numpy.linalg.eig}. (10 points) \\

\item Inverse Power Method:

Write a function \texttt{inverse\_power\_method(A,x)}, which takes as input matrix $\mathbf{A}$ and a vector $\mathbf{x}$, and uses inverse power method to calculate the smallest (in absolute value) eigenvalue and corresponding eigenvector. Solve for the smallest (in absolute value) eigenvalue and corresponding eigenvector for the matrix from (a). Use the same initial eigenvector guesses as (a). 
\begin{enumerate}[label=\roman*.]
    \item Plot the computed/estimated eigenvalue with respect to iterations. 
    \item Report how many iterations do you need for it to converge to the smallest eigenvalue.
    \item  Report the final eigenvalue and eigenvector you get. Match your answer with the results generated by the numpy API \texttt{numpy.linalg.eig}.\\
\end{enumerate}

\emph{Note}: You only need to look at magnitudes of eigenvalues. Use an absolute tolerance of $10^{-6}$ between eigenvalue output of previous and current iteration as stopping criteria. You may also need to normalize the final eigenvector to match with output of numpy API \texttt{numpy.linalg.eig}. (10 points)\\

\end{enumerate}
% \vspace{0.3cm}
\textbf{Solution:}
See attached notebook.

%%%%%%%%%%%%%%%%%%%%
\clearpage
\item[] \textbf{Question 4: Face Recognition with Eigenfaces (30 points + 10 Bonus Points)} \\ 
%%%%%%%%%%%%%%%%%%%%

Solve the problem in the \href{https://drive.google.com/file/d/1cOb-zLjKqcIPwESdG55NyQ26GIms09Z2/view?usp=drive_link}{google colab}. Please make a copy of the notebook and solve it there.

\textbf{Goal}: Perform face recognition on the \emph{Labeled Faces in the Wild} dataset using PyTorch.

\textbf{Dataset Information}: \href{http://vis-www.cs.umass.edu/lfw/}{Labeled Faces in the Wild} dataset consists of face photographs designed for studying the problem of unconstrained face recognition. The original dataset contains more than 13,000 images of faces collected from the web.

\textbf{Tasks}:
\begin{itemize}
    \item First, perform Principal Component Analysis (PCA) on the image dataset.
    \item Using PCA, extract the Top $k$ principal components (eigenvalues).
    \item Reconstruction of faces from these \textbf{eigenvalues} will give us the \textbf{eigen-faces} which are the most representative features of most of the images in the dataset.
    \item \textbf{BONUS}: Finally, train a simple PyTorch Neural Network model on the modified image dataset. This trained model will be used for prediction and evaluation on a test set.
\end{itemize}

Problem Description
\begin{enumerate}[label=(\alph*)]
\item Preprocessing: Using the \texttt{train\_test\_split} API from  \texttt{sklearn}, split the data into train and test dataset in the ratio 3:1. Use  \texttt{random\_state=42}.

For better performance, normalize the features which can have different ranges with huge values. (As all our features here are in the range [0, 255],  it is not explicitly needed here. However, it is a good exercise.)

Use the  \texttt{StandardScaler} class from  \texttt{sklearn} and use that to normalize  \texttt{X\_train} and  \texttt{X\_test}. Validate and show your result by printing the first 5 features of 5 images of  \texttt{X\_train} (This result can vary from PC to PC). (10 points)

\item Dimensionality reduction: 
Use the PCA API from \texttt{sklearn} to extract the top 100 principal components of the image matrix and fit it on the training dataset.

Visualize some of the top few components as an image (eigenfaces). (10 points)

\item Face reconstruction: 
Reconstruct an image from its point projected on the principal component basis. Project the first three faces on the eigenvector basis using PCA models trained with varying number of principal components. Using the projected points, reconstruct the faces, and visualize  the images.

Your final output should be a $(3\times 5)$ image matrix, where the rows are the data points, and the columns correspond to original image and reconstructed image for $\text{n\_components} =[10,100,150,500]$. (10 points)

\item Prediction \textbf{(BONUS)}: Train a neural network classifier in \textbf{PyTorch} on the transformed dataset. 
Note: For PyTorch reference see \href{https://pytorch.org/docs/stable/index.html}{documentation} (10 points) \\
\end{enumerate}
\textbf{Solution:}
See attached notebook.
\end{enumerate}

% \end{enumerate}

\includepdf[pages=-]{HW3_collab.pdf}

\end{document}